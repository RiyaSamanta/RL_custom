{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom environment\n",
    "class ShowerEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(9)\n",
    "        print(\"*****************\", self.action_space)\n",
    "        self.observation_space = Discrete(9)\n",
    "        self.path = []  # Start with the initial state\n",
    "        self.state = 0\n",
    "        self.episode = 100\n",
    "        self.reward_matrix = np.array([\n",
    "            [-10, 10, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [-20, -10, -50, 0, 10, 0, 0, 0, 0],\n",
    "            [0, 0, -10, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, -10, 0, 0, 0, 0, 0],\n",
    "            [0, -20, 0, -50, -10, -50, 0, 10, 0],\n",
    "            [0, 0, 0, 0, 0, -10, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, -10, -20, 0],\n",
    "            [0, 0, 0, 0, -20, 0, 50, -10, 100],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, -10],\n",
    "        ])\n",
    "        \n",
    "    def step(self, action):\n",
    "        new_state_path = -1\n",
    "        if self.reward_matrix[self.state, action] in [-50, 100]:\n",
    "            newstate = action\n",
    "            reward = self.reward_matrix[self.state, action]\n",
    "            self.state = newstate\n",
    "            done = True\n",
    "            new_state_path =newstate\n",
    "        else:\n",
    "            if self.state != action:\n",
    "                if self.reward_matrix[self.state, action] != 0:\n",
    "                    newstate = action\n",
    "                    reward = self.reward_matrix[self.state, action]\n",
    "                    self.state = newstate\n",
    "                    new_state_path =newstate\n",
    "                else:\n",
    "                    newstate = self.state\n",
    "                    reward = self.reward_matrix[self.state, action]\n",
    "                    self.state = newstate\n",
    "                    new_state_path =newstate\n",
    "            else:\n",
    "                newstate = self.state\n",
    "                reward = self.reward_matrix[self.state, action]\n",
    "                self.state = newstate\n",
    "                new_state_path =newstate\n",
    "                \n",
    "        self.episode -= 1\n",
    "        if self.episode <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        info = {}\n",
    "        self.path.append(new_state_path)\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        print(\"Path taken:\", self.path)  # Print the path taken\n",
    "        self.path = []  # Reset the path for the next episode\n",
    "        self.state = 0\n",
    "        self.episode = 100\n",
    "        return self.state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environment with gym\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='ShowerEnv-v0',\n",
    "    entry_point='__main__:ShowerEnv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** Discrete(9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bidyut/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = DummyVecEnv([lambda: gym.make('ShowerEnv-v0')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bidyut/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/bidyut/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/bidyut/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'int'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/home/bidyut/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path taken: []\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[1;32m     12\u001b[0m \u001b[39m# print(\"old state--->\",state)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# print(\"Action--->\",action)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m n_state, reward, done, info\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     15\u001b[0m \u001b[39m# print(\"New state--->\",n_state)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# print(\"reward== \",reward)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m score\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mreward\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m     58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_idx]\u001b[39m.\u001b[39mstep(\n\u001b[0;32m---> 59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "# states=[]\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        # print(\"old state--->\",state)\n",
    "        # print(\"Action--->\",action)\n",
    "        n_state, reward, done, info= env.step(action)\n",
    "        # print(\"New state--->\",n_state)\n",
    "        # print(\"reward== \",reward)\n",
    "        score+=reward\n",
    "        # states.append(state)\n",
    "        # states.append(n_state)\n",
    "        \n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "    # print(\"States covered-->\",states)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment to start a new episode\n",
    "obs = env.reset()\n",
    "\n",
    "# Initialize the path with the starting state\n",
    "path = [obs]\n",
    "score=0\n",
    "# Run the episode\n",
    "done = False\n",
    "while not done:\n",
    "    # Use the trained model to predict the next action\n",
    "    action, _states = model.predict(obs)\n",
    "    \n",
    "    # Take the action in the environment\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    score+=reward\n",
    "    # Append the new state to the path\n",
    "    path.append(obs)\n",
    "\n",
    "print(\"Predicted path:\", path)\n",
    "print(\"cumulative reward\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
